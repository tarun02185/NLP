{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "P8jkar4ALR5f"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from __future__ import annotations\n",
        "import collections\n",
        "import itertools\n",
        "import math\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, deque\n",
        "from typing import Dict, Tuple, Deque, Generator, List\n",
        "\n",
        "SENTENCES_FILE = \"/content/drive/MyDrive/dataset/sentences_of_hindi_corpus_1.txt\"\n",
        "SENTENCE_LIMIT = 1000000\n",
        "MAX_N = 4\n",
        "ADD_K = 0.5\n",
        "START_TOKEN = \"<s>\"\n",
        "END_TOKEN = \"</s>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "82owPvcARDpP"
      },
      "outputs": [],
      "source": [
        "def stream_sentences_with_boundaries(file_path: str, limit: int) -> Generator[str, None, None]:\n",
        "    sentences_read = 0\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for line in itertools.islice(f, limit):\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    yield START_TOKEN\n",
        "\n",
        "                    for tok in line.split():\n",
        "                        t = tok.strip()\n",
        "                        if t:\n",
        "                            yield t\n",
        "\n",
        "                    yield END_TOKEN\n",
        "                    sentences_read += 1\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        return\n",
        "\n",
        "    print(f\"Successfully processed {sentences_read} sentences.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "v-yg0zrORDlw"
      },
      "outputs": [],
      "source": [
        "def mle_conditional(ngram: Tuple[str, ...], counts_n: Dict[Tuple[str, ...], int], counts_prev: Dict[Tuple[str, ...], int] | int, total_tokens: int) -> float:\n",
        "    \"\"\"MLE Conditional Probability: P(w_n | h) = C(h, w_n) / C(h)\"\"\"\n",
        "    count_ngram = counts_n.get(ngram, 0)\n",
        "\n",
        "    if len(ngram) == 1:\n",
        "        # Unigram context is total_tokens (N)\n",
        "        denom = total_tokens\n",
        "    else:\n",
        "        # N-gram context C(h) is retrieved from the counts_prev dictionary\n",
        "        hist = ngram[:-1]\n",
        "        denom = counts_prev.get(hist, 0)\n",
        "\n",
        "    return count_ngram / denom if denom else 0.0\n",
        "\n",
        "\n",
        "def add_one_conditional(ngram: Tuple[str, ...], counts_n: Dict[Tuple[str, ...], int], counts_prev: Dict[Tuple[str, ...], int] | int, vocab_size: int, total_tokens: int) -> float:\n",
        "    \"\"\"Add-One Smoothing: P*(w_n | h) = (C(h, w_n) + 1) / (C(h) + V)\"\"\"\n",
        "    count_ngram = counts_n.get(ngram, 0)\n",
        "\n",
        "    if len(ngram) == 1:\n",
        "        count_context = total_tokens\n",
        "    else:\n",
        "        hist = ngram[:-1]\n",
        "        count_context = counts_prev.get(hist, 0)\n",
        "\n",
        "    denom = count_context + vocab_size\n",
        "    return (count_ngram + 1) / denom if denom else 0.0\n",
        "\n",
        "\n",
        "def add_k_conditional(ngram: Tuple[str, ...], counts_n: Dict[Tuple[str, ...], int], counts_prev: Dict[Tuple[str, ...], int] | int, vocab_size: int, k: float, total_tokens: int) -> float:\n",
        "    \"\"\"Add-K Smoothing: P*(w_n | h) = (C(h, w_n) + K) / (C(h) + K * V)\"\"\"\n",
        "    count_ngram = counts_n.get(ngram, 0)\n",
        "\n",
        "    if len(ngram) == 1:\n",
        "        count_context = total_tokens\n",
        "    else:\n",
        "        hist = ngram[:-1]\n",
        "        count_context = counts_prev.get(hist, 0)\n",
        "\n",
        "    denom = count_context + (k * vocab_size)\n",
        "    return (count_ngram + k) / denom if denom else 0.0\n",
        "\n",
        "\n",
        "def token_type_score(ngram: Tuple[str, ...], counts_n: Dict[Tuple[str, ...], int]) -> float:\n",
        "    \"\"\"Add Token Type Smoothing (Custom Score): C(Ngram) + len(unique characters in predicted word)\"\"\"\n",
        "    count_ngram = counts_n.get(ngram, 0)\n",
        "    predicted = ngram[-1]\n",
        "    tts_score_part = float(len(set(predicted)))\n",
        "    return count_ngram + tts_score_part, tts_score_part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ow77s4bTRDi0"
      },
      "outputs": [],
      "source": [
        "def print_demonstration(n: int, counts_n: Dict[Tuple[str, ...], int], counts_prev: Dict[Tuple[str, ...], int] | int, vocab_size: int, total_tokens: int, gram: Tuple[str, ...], context: Tuple[str, ...], label: str):\n",
        "\n",
        "    count_ngram = counts_n.get(gram, 0)\n",
        "\n",
        "    # Correctly retrieve context count for printing (either N or count from dict)\n",
        "    if n == 1:\n",
        "        count_context = total_tokens\n",
        "    else:\n",
        "        count_context = counts_prev.get(context, 0)\n",
        "\n",
        "    # MLE\n",
        "    mle_p = mle_conditional(gram, counts_n, counts_prev, total_tokens)\n",
        "\n",
        "    # Add-One\n",
        "    add1_p = add_one_conditional(gram, counts_n, counts_prev, vocab_size, total_tokens)\n",
        "\n",
        "    # Add-K\n",
        "    addk_p = add_k_conditional(gram, counts_n, counts_prev, vocab_size, ADD_K, total_tokens)\n",
        "\n",
        "    # Token Type Score\n",
        "    tts, tts_score_part = token_type_score(gram, counts_n)\n",
        "\n",
        "    print(f\"\\n--- {n}-gram Model ({label}) ---\")\n",
        "    print(f\"  Counts: C(Ngram)={count_ngram}, C(Context)={count_context}\")\n",
        "    print(f\"  Unsmoothed (MLE): {mle_p:.10f}\")\n",
        "    print(f\"  Add-One (k=1): {add1_p:.10f}\")\n",
        "    print(f\"  Add-K (k={ADD_K}): {addk_p:.10f}\")\n",
        "    print(f\"  Add Token Type Score: {tts:.4f} (C(Ngram) + {tts_score_part:.2f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blO3t5N-RDgb",
        "outputId": "20b74ad7-2e47-4836-f23c-850f7bc99637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 1000000 sentences.\n",
            "\n",
            "==================================================\n",
            "MODEL SUMMARY\n",
            "Total tokens (N): 19112465; Vocabulary size (V): 260592\n",
            "Unique 1-grams: 260592\n",
            "Unique 2-grams: 3161479\n",
            "Unique 3-grams: 8887725\n",
            "Unique 4-grams: 13350139\n",
            "==================================================\n",
            "\n",
            "--- 1-gram Model (P(खाना)) ---\n",
            "  Counts: C(Ngram)=1503, C(Context)=19112465\n",
            "  Unsmoothed (MLE): 0.0000786398\n",
            "  Add-One (k=1): 0.0000776336\n",
            "  Add-K (k=0.5): 0.0000781333\n",
            "  Add Token Type Score: 1506.0000 (C(Ngram) + 3.00)\n",
            "\n",
            "--- 2-gram Model (P(खाना | आज)) ---\n",
            "  Counts: C(Ngram)=1, C(Context)=15657\n",
            "  Unsmoothed (MLE): 0.0000638692\n",
            "  Add-One (k=1): 0.0000072398\n",
            "  Add-K (k=0.5): 0.0000102773\n",
            "  Add Token Type Score: 4.0000 (C(Ngram) + 3.00)\n",
            "\n",
            "--- 3-gram Model (P(खाना | मैं, आज)) ---\n",
            "  Counts: C(Ngram)=0, C(Context)=68\n",
            "  Unsmoothed (MLE): 0.0000000000\n",
            "  Add-One (k=1): 0.0000038364\n",
            "  Add-K (k=0.5): 0.0000038354\n",
            "  Add Token Type Score: 3.0000 (C(Ngram) + 3.00)\n",
            "\n",
            "--- 4-gram Model (P(खाना | <s>, मैं, आज)) ---\n",
            "  Counts: C(Ngram)=0, C(Context)=19\n",
            "  Unsmoothed (MLE): 0.0000000000\n",
            "  Add-One (k=1): 0.0000038371\n",
            "  Add-K (k=0.5): 0.0000038369\n",
            "  Add Token Type Score: 3.0000 (C(Ngram) + 3.00)\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    counts: Dict[int, Dict[Tuple[str, ...], int]] = {n: defaultdict(int) for n in range(1, MAX_N + 1)}\n",
        "    total_tokens = 0\n",
        "    vocab = set()\n",
        "    window: Deque[str] = deque(maxlen=MAX_N - 1)\n",
        "\n",
        "    # 1. Load Data (Q1) and Count N-grams (Q2)\n",
        "    for tok in stream_sentences_with_boundaries(SENTENCES_FILE, SENTENCE_LIMIT):\n",
        "        total_tokens += 1\n",
        "        vocab.add(tok)\n",
        "\n",
        "        counts[1][(tok,)] += 1\n",
        "\n",
        "        if MAX_N > 1:\n",
        "            hist = list(window)\n",
        "            hl = len(hist)\n",
        "\n",
        "            for n in range(2, MAX_N + 1):\n",
        "                needed = n - 1\n",
        "                if hl >= needed:\n",
        "                    gram = tuple(hist[-needed:] + [tok])\n",
        "                    counts[n][gram] += 1\n",
        "\n",
        "        window.append(tok)\n",
        "\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MODEL SUMMARY\")\n",
        "    print(f\"Total tokens (N): {total_tokens}; Vocabulary size (V): {vocab_size}\")\n",
        "    for n in range(1, MAX_N + 1):\n",
        "        print(f\"Unique {n}-grams: {len(counts[n])}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 2. Demonstration of Probability Calculation (Q2)\n",
        "\n",
        "    # Example N-grams to test\n",
        "    unigram = (\"खाना\",)\n",
        "    bigram = (\"आज\", \"खाना\")\n",
        "    trigram = (\"मैं\", \"आज\", \"खाना\")\n",
        "    quadrigram = (START_TOKEN, \"मैं\", \"आज\", \"खाना\")\n",
        "\n",
        "    examples = [\n",
        "        (1, unigram, tuple(), \"P(खाना)\"),\n",
        "        (2, bigram, (\"आज\",), \"P(खाना | आज)\"),\n",
        "        (3, trigram, (\"मैं\", \"आज\"), \"P(खाना | मैं, आज)\"),\n",
        "        (4, quadrigram, (START_TOKEN, \"मैं\", \"आज\"), f\"P(खाना | {START_TOKEN}, मैं, आज)\"),\n",
        "    ]\n",
        "\n",
        "    for n, gram, context, label in examples:\n",
        "        # Pass total_tokens for N=1, or the counts dictionary for N>1\n",
        "        counts_prev = total_tokens if n == 1 else counts[n-1]\n",
        "\n",
        "        print_demonstration(\n",
        "            n, counts[n], counts_prev, vocab_size, total_tokens, gram, context, label\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
