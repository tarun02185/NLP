{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ddb3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d72be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('../Lab5/train.parquet')\n",
    "val_df = pd.read_parquet('../Lab5/validation.parquet')\n",
    "test_df = pd.read_parquet('../Lab5/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c571139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(df, column=\"sentences\"):\n",
    "    \"\"\"Extract plain text sentences from [{'text': ...}] structures.\"\"\"\n",
    "    return [d[\"text\"] for row in df[column] for d in row]\n",
    "\n",
    "train_sentences = extract_sentences(train_df)\n",
    "val_sentences   = extract_sentences(val_df)\n",
    "test_sentences  = extract_sentences(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b600f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Splits a sentence into words, converts to lowercase, and removes punctuation.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a string\n",
    "    if not isinstance(sentence, str):\n",
    "        return []\n",
    "    tokens = re.sub(r'[^\\w\\s]', '', sentence.lower()).split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1709c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(train_corpus_tokens):\n",
    "    \"\"\"\n",
    "    Calculates IDF scores for a vocabulary learned from the training corpus.\n",
    "    :param train_corpus_tokens: A list of tokenized sentences from the training data.\n",
    "    :return: A tuple containing (vocabulary, idf_scores_dict).\n",
    "    \"\"\"\n",
    "    num_docs = len(train_corpus_tokens)\n",
    "    \n",
    "    # Build vocabulary and document frequency (DF) in one pass\n",
    "    vocab = set()\n",
    "    doc_freq = {}\n",
    "    for doc in train_corpus_tokens:\n",
    "        unique_words_in_doc = set(doc)\n",
    "        for word in unique_words_in_doc:\n",
    "            vocab.add(word)\n",
    "            doc_freq[word] = doc_freq.get(word, 0) + 1\n",
    "            \n",
    "    sorted_vocab = sorted(list(vocab))\n",
    "    \n",
    "    # Calculate IDF scores using the document frequencies\n",
    "    idf_scores = {\n",
    "        word: math.log(num_docs / (doc_freq.get(word, 0) + 1))\n",
    "        for word in sorted_vocab\n",
    "    }\n",
    "    \n",
    "    print(f\"Learned vocabulary with {len(sorted_vocab)} words and calculated IDF scores.\")\n",
    "    return sorted_vocab, idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f69801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(sentence_tokens):\n",
    "    \"\"\"\n",
    "    Calculates TF scores for a single tokenized sentence.\n",
    "    :param sentence_tokens: A list of words for one sentence.\n",
    "    :return: A dictionary of {word: tf_score}.\n",
    "    \"\"\"\n",
    "    doc_len = len(sentence_tokens)\n",
    "    if doc_len == 0:\n",
    "        return {}\n",
    "        \n",
    "    word_counts = {}\n",
    "    for word in sentence_tokens:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "    tf_scores = {\n",
    "        word: count / doc_len\n",
    "        for word, count in word_counts.items()\n",
    "    }\n",
    "    return tf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b0445c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [tokenize(s) for s in train_sentences]\n",
    "tokenized_val = [tokenize(s) for s in val_sentences]\n",
    "tokenized_test = [tokenize(s) for s in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ecabce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned vocabulary with 404480 words and calculated IDF scores.\n"
     ]
    }
   ],
   "source": [
    "vocabulary, idf_values = compute_idf(tokenized_train)\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Keep all your other functions like tokenize, compute_idf, compute_tf the same)\n",
    "\n",
    "def create_tfidf_vectors(corpus_tokens, vocabulary, idf_scores, word_map):\n",
    "    \"\"\"\n",
    "    Creates a memory-efficient sparse representation of TF-IDF vectors.\n",
    "    Instead of a list of lists, it returns a list of dictionaries.\n",
    "    \"\"\"\n",
    "    tfidf_vector_list = []\n",
    "    \n",
    "    for doc_tokens in corpus_tokens:\n",
    "        tf_scores_doc = compute_tf(doc_tokens)\n",
    "        \n",
    "        # This dictionary will only store non-zero values for this sentence.\n",
    "        sparse_vector = {} \n",
    "        \n",
    "        for word, tf_val in tf_scores_doc.items():\n",
    "            if word in word_map:\n",
    "                index = word_map[word]\n",
    "                # Store the TF-IDF score with its index as the key\n",
    "                sparse_vector[index] = tf_val * idf_scores[word]\n",
    "                \n",
    "        tfidf_vector_list.append(sparse_vector)\n",
    "        \n",
    "    return tfidf_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220048de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorizing all datasets using a sparse representation...\n",
      "...Done.\n",
      "\n",
      "--- Sparse Vectorization Summary ---\n",
      "Number of training vectors: 3257770\n",
      "Number of validation vectors: 3126\n",
      "Number of testing vectors: 3098\n",
      "\n",
      "First sparse vector (train): {288827: 0.32601781907202326, 311956: 0.35742971753019537, 354700: 0.2593349265147189, 246464: 0.32490371350288866, 148141: 0.03218466422406572, 355570: 0.37425119204449664, 342172: 0.13805699829218238, 248304: 0.38494336427150666, 219096: 0.3307244065191351, 361523: 0.0615495768521628, 374624: 0.33775986574192923, 161652: 0.10967732240295959, 334478: 0.10696103527814818, 266652: 0.2424960061341519, 234229: 0.18263677465871955, 222407: 0.31430469557883967, 194706: 0.12447864731312021, 332846: 0.09521231307741765, 388932: 0.028581179856532368}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVectorizing all datasets using a sparse representation...\")\n",
    "X_train_tfidf_sparse = create_tfidf_vectors(tokenized_train, vocabulary, idf_values, word_to_index)\n",
    "X_val_tfidf_sparse = create_tfidf_vectors(tokenized_val, vocabulary, idf_values, word_to_index)\n",
    "X_test_tfidf_sparse = create_tfidf_vectors(tokenized_test, vocabulary, idf_values, word_to_index)\n",
    "print(\"...Done.\")\n",
    "\n",
    "print(\"\\nSparse Vectorization Summary\")\n",
    "print(f\"Number of training vectors: {len(X_train_tfidf_sparse)}\")\n",
    "print(f\"Number of validation vectors: {len(X_val_tfidf_sparse)}\")\n",
    "print(f\"Number of testing vectors: {len(X_test_tfidf_sparse)}\")\n",
    "\n",
    "print(f\"\\nFirst sparse vector (train): {X_train_tfidf_sparse[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
