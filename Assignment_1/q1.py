# -*- coding: utf-8 -*-
"""q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FyczYfbje2ginVJPal472K31J-VAOWec
"""

from datasets import load_dataset
dataset = load_dataset("ai4bharat/IndicCorpV2", "indiccorp_v2", streaming = "True", split="hin_Deva")

import re

def hindi_sentence_tokenizer(text):
    return re.split(r'(?<=[।!?])\s+', text)


def hindi_word_tokenizer(text):
    return re.findall(r'[\u0900-\u097F]+|\d+|[.,!?]', text)


N = 10
for i, example in enumerate(dataset):
    if i >= N:
        break
    text = example['text']
    sentences = hindi_sentence_tokenizer(text)
    for s in sentences:
        words = hindi_word_tokenizer(s)
        print(words)

tokens_file = "tokens_of_hindi_corpus.txt"
sentences_file = "sentences_of_hindi_corpus.txt"


N = 100


total_sentences = 0
total_words = 0
total_chars = 0
unique_word_tokens = set()

processed_paragraphs = 0

with open(tokens_file, "w", encoding="utf-8") as tf, open(sentences_file, "w", encoding="utf-8") as sf:
    for example in dataset:
        if processed_paragraphs >= N:
            break

        text = example.get("text", "").strip()
        if not text:
            continue

        text = re.sub(r'\s+', ' ', text).strip()

        sentences = hindi_sentence_tokenizer(text)
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            words = hindi_word_tokenizer(sentence)
            if not words:
                continue

            tokenized_sentence = " ".join(words)
            sf.write(tokenized_sentence + "\n")

            total_sentences += 1
            total_words += len(words)
            total_chars += sum(len(w) for w in words)

            for w in words:
                tf.write(w + "\n")
                unique_word_tokens.add(w)

        processed_paragraphs += 1




avg_sentence_length = total_words / total_sentences
avg_word_length = total_chars / total_words
ttr = len(unique_word_tokens) / total_words



print(f"1. Total number of sentences: {total_sentences}")
print(f"2. Total number of words: {total_words}")
print(f"3a. Total characters : {total_chars}")
print(f"4. Average sentence length : {avg_sentence_length:.2f}")
print(f"5. Average word length : {avg_word_length:.2f}")
print(f"6. Type/Token Ratio (TTR): {ttr:.4f}")
print(f"   → Unique word tokens: {len(unique_word_tokens)}")

